{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bkosa2/anaconda3/envs/rnp/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import functorch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bkosa2/RNP/RNP_PyTorch/Pytorch_hypernets/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Important for using GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "# input_size = 784  # 28x28\n",
    "# synthnet_hidden_size = 100\n",
    "# num_classes = 10\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../data/mnist_data', \n",
    "                                                          download=True, \n",
    "                                                          train=True,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                          ])), \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# download and transform test dataset\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('../data/mnist_data', \n",
    "                                                          download=True, \n",
    "                                                          train=False,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                          ])), \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperNetwork(torch.nn.Module):\n",
    "    def __init__(self, hypnet: torch.nn.Module, synthnet: torch.nn.Module):\n",
    "        # hypnet is the network that takes an embedding z and produces the parameters of synthnet\n",
    "        # synthnet is the network that takes input x and produces h\n",
    "        super().__init__()\n",
    "        s_func, s_params0 = functorch.make_functional(synthnet)\n",
    "\n",
    "        # store the information about the parameters\n",
    "        self._sp_shapes = [sp.shape for sp in s_params0]\n",
    "\n",
    "        # These are the index offsets for each parameter (e.g. set of weights between\n",
    "        # each layer and set of biases for each layer)\n",
    "        self._sp_offsets = np.array([0, *np.cumsum([sp.numel() for sp in s_params0])])\n",
    "\n",
    "        # make the synthnet_func to accept batched parameters\n",
    "        synthnet_func = functorch.vmap(s_func)\n",
    "        # a workaround of functorch's bug #793\n",
    "        # self._synthnet_batched_func = [synthnet_func]\n",
    "        self._synthnet_batched_func = synthnet_func\n",
    "        self._hypnet = hypnet\n",
    "\n",
    "    def forward(self, x: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch_size, nx), z: (batch_size, nz)\n",
    "        params = self._hypnet(z)  # params: (batch_size, nparams_tot)\n",
    "\n",
    "        # rearrange params to have the same shape as the synthnet params, except on the batch dimension\n",
    "        # print(f\"self._sp_offsets: {self._sp_offsets}\")\n",
    "        params_lst = []\n",
    "        for i, shape in enumerate(self._sp_shapes):\n",
    "            j0, j1 = self._sp_offsets[i], self._sp_offsets[i + 1]\n",
    "            params_lst.append(params[..., j0:j1].reshape(-1, *shape))\n",
    "\n",
    "        # print(f\"params_lst: {params_lst}\")\n",
    "\n",
    "        # apply the function to the batched parameters and x\n",
    "        h = self._synthnet_batched_func(params_lst, x)\n",
    "        return h\n",
    "        # return params_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Building an linear encoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # 784 ==> 9\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28 * 28, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 36),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(36, 18),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(18, 9)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array([0, *np.cumsum([sp.numel() for sp in a])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthnet = torch.nn.Sequential(\n",
    "    torch.nn.Linear(28*28, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, 10),\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, synthnet_params = functorch.make_functional(synthnet)\n",
    "n_synthnet_params = sum([p.numel() for p in synthnet_params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([100, 784])\n",
      "torch.Size([100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n",
      "79510\n"
     ]
    }
   ],
   "source": [
    "# Ben's Code - Just me peeking inside the synthnet_params to understand what PyTorch Module params actually are under the hood. synthnet is just a nn with 1 hidden layer.\n",
    "print(len(synthnet_params))\n",
    "print(synthnet_params[0].size())  # The weights for the connections from input layer (784 units) to hidden layer (100 units)\n",
    "print(synthnet_params[1].size())  # The biases for each of the 100 units in the hidden layer???\n",
    "print(synthnet_params[2].size())  # The weights for the connections from the hidden layer (100 units) to the output layer (10 units)\n",
    "print(synthnet_params[3].size())  # The biases for each of the 10 units in the output layer???\n",
    "print(n_synthnet_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! todo \n",
    "hypnet = torch.nn.Sequential(\n",
    "    torch.nn.Linear(9, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, n_synthnet_params),\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = HyperNetwork(hypnet, synthnet).to(device)\n",
    "encoder = Encoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2939e-01, -1.0539e+00,  1.2904e+00, -8.5967e-01, -9.5300e-01,\n",
       "          4.1317e-01,  1.9479e-01, -1.6215e+00,  4.4721e-01,  4.0094e-01],\n",
       "        [-4.1470e-02,  1.2322e+00,  1.2806e+00, -1.2209e-01, -5.6639e-01,\n",
       "          7.0445e-01, -6.7706e-02, -3.2500e-01, -1.2897e-01, -6.0678e-01],\n",
       "        [ 1.5933e-01, -4.2136e-01,  3.6571e-01, -6.8924e-01,  3.4137e-02,\n",
       "         -1.7620e-01,  6.7944e-01, -1.3010e+00,  5.4643e-01,  4.5805e-01],\n",
       "        [-2.0620e-01,  6.4094e-01,  5.7286e-01, -3.1576e-01,  1.6518e-01,\n",
       "         -5.5355e-01,  1.1691e-01, -3.1885e-02, -5.1861e-01, -1.0369e+00],\n",
       "        [ 6.0133e-01, -2.4626e-01, -8.8934e-01, -7.9846e-01, -9.3119e-01,\n",
       "          6.6587e-01,  3.8584e-01, -1.2794e-01,  2.4409e-01, -6.5646e-01],\n",
       "        [ 2.8435e-01,  1.9275e-01,  3.9300e-01, -3.0803e-01, -1.3049e-01,\n",
       "         -7.7536e-02,  4.9321e-01, -7.1867e-01, -3.6915e-01,  1.0619e-01],\n",
       "        [ 1.3212e-01, -5.8051e-01,  3.0817e-01, -4.7559e-01, -4.5391e-01,\n",
       "          3.9928e-01,  1.5846e-01, -6.0459e-01,  8.6443e-01, -2.3301e-01],\n",
       "        [ 1.9998e-01,  6.0315e-02,  6.1491e-01, -2.7778e-01, -3.8421e-01,\n",
       "         -3.5932e-01,  4.8191e-01, -8.3716e-01, -2.6262e-01,  2.8366e-01],\n",
       "        [-1.4128e-01,  4.0423e-01,  4.8374e-01, -3.6890e-01, -7.3703e-01,\n",
       "         -3.5838e-04,  3.3407e-01, -8.4917e-01,  2.5763e-01,  2.6291e-01],\n",
       "        [-4.8675e-01,  7.4609e-02,  7.3589e-01, -4.5667e-01,  2.0012e-01,\n",
       "         -2.4748e-01,  2.1139e-01, -3.9300e-01, -3.2083e-01,  2.3433e-01],\n",
       "        [ 7.3217e-01, -2.5309e-01, -3.8763e-01, -3.9132e-01, -9.7214e-01,\n",
       "          2.5519e-01,  3.9498e-01, -7.5966e-02,  8.0086e-01, -6.5614e-01],\n",
       "        [ 1.5674e+00, -3.1400e-01, -1.0468e+00, -8.9660e-01, -1.7753e-01,\n",
       "          4.4508e-01,  3.6776e-01, -9.7964e-01,  4.7123e-01, -2.4184e-01],\n",
       "        [ 9.6437e-02, -1.3500e-01,  2.6885e-01, -1.3802e-01, -3.2611e-01,\n",
       "          3.1845e-01,  1.7818e-01,  8.4921e-02,  2.1795e-01, -3.0979e-01],\n",
       "        [ 1.8728e-01, -2.5015e-01, -2.0518e-01, -5.2155e-01, -8.0061e-01,\n",
       "          2.9581e-01, -1.8849e-02, -6.0600e-01,  1.6236e-01,  8.5002e-01],\n",
       "        [ 1.0978e-01,  8.2079e-01,  9.3391e-03, -1.6145e-01, -2.2828e-01,\n",
       "         -1.8213e-01, -1.6929e-01, -3.7403e-01, -3.5860e-01, -3.7038e-01],\n",
       "        [ 5.2051e-01, -1.9506e-01, -5.9594e-02, -4.1463e-01, -6.7801e-01,\n",
       "          6.6370e-01,  1.3629e-01, -1.2021e+00,  5.6708e-01,  7.5208e-01],\n",
       "        [-3.7340e-01, -5.5107e-01,  6.4044e-01, -1.3237e+00, -2.9680e-01,\n",
       "          7.4486e-01,  3.5055e-01, -1.8152e+00,  8.0025e-01,  5.6054e-01],\n",
       "        [ 7.9452e-01, -3.1963e-02, -1.8322e-01, -1.3995e-01, -1.0166e+00,\n",
       "          6.1002e-01,  2.0965e-01,  4.1570e-02, -1.2071e-02, -9.6684e-02],\n",
       "        [ 3.1208e-01,  3.4052e-01,  6.9688e-01, -5.2868e-01,  1.4756e-01,\n",
       "          1.3522e-01,  2.2995e-01, -1.3633e+00,  3.0759e-01,  1.6232e-01],\n",
       "        [ 9.4241e-03,  7.1503e-02,  9.4388e-01, -3.2449e-01, -2.7715e-01,\n",
       "         -2.6420e-01,  1.1602e-01, -1.0177e+00,  7.2059e-01, -3.5730e-01],\n",
       "        [ 2.6614e-01, -2.6849e-01,  3.5801e-01, -5.7297e-01, -3.8466e-01,\n",
       "          2.6788e-01,  4.6210e-01, -8.4100e-01,  4.8574e-01, -4.5963e-01],\n",
       "        [-3.1123e-01,  4.5821e-01,  9.9571e-01, -4.8052e-01, -1.4736e-01,\n",
       "         -3.3273e-02,  7.0716e-01, -5.9194e-01, -1.8517e-01, -4.8750e-01],\n",
       "        [ 3.9902e-01,  3.5285e-01,  5.2668e-01, -4.7377e-02, -4.4185e-01,\n",
       "         -3.9151e-01,  6.4169e-01, -7.1477e-01,  2.8312e-01, -5.2483e-01],\n",
       "        [-1.1371e-01,  1.0399e-01,  1.6552e-01, -1.7830e-01, -1.8489e-01,\n",
       "          1.3268e-03,  6.4063e-01, -3.5061e-01,  2.8894e-01, -4.7471e-01],\n",
       "        [ 1.4001e-01, -4.7161e-01,  1.1172e-01, -7.3845e-02, -3.9068e-01,\n",
       "          5.8145e-01,  4.1527e-01, -2.4259e-01,  4.8873e-01, -2.5020e-01],\n",
       "        [-1.8998e-01,  9.0072e-01, -6.9950e-02, -3.3740e-01, -1.0530e-01,\n",
       "         -4.9855e-02,  3.1411e-01,  9.7617e-02,  1.7621e-01, -7.1360e-01],\n",
       "        [ 5.1053e-01,  2.4561e-01,  3.2441e-02,  2.2865e-01, -2.2077e-01,\n",
       "          1.9478e-01,  1.4410e-02, -5.1532e-01, -3.1287e-01, -6.0678e-03],\n",
       "        [-1.0105e-01, -5.5189e-01,  3.1202e-01, -5.6233e-01, -3.2504e-01,\n",
       "          8.4659e-01,  4.4369e-01, -7.8001e-01,  6.4818e-01, -3.3118e-01],\n",
       "        [ 1.9499e-01, -7.6319e-01, -5.9079e-02, -9.4983e-01, -4.2189e-01,\n",
       "          2.5305e-01,  4.1450e-01, -3.7780e-02,  3.3622e-01, -8.4048e-01],\n",
       "        [ 4.2367e-01, -6.4335e-01,  7.3967e-01, -9.0273e-01, -7.4366e-01,\n",
       "          7.2993e-01, -4.6835e-02, -1.9708e+00,  7.9914e-01,  6.2432e-01],\n",
       "        [-5.5844e-01, -2.0941e-01,  4.4657e-01, -2.9202e-01, -4.0862e-01,\n",
       "         -2.7995e-01,  1.3373e-01, -2.8627e-01, -3.4248e-02, -4.3903e-01],\n",
       "        [ 1.3026e-02,  4.5422e-01,  7.5240e-01, -2.0110e-01, -5.2829e-01,\n",
       "         -2.8819e-01,  2.0795e-01, -1.5805e+00, -2.8409e-01, -3.3941e-01],\n",
       "        [-1.0228e-01,  1.5511e-01,  1.8124e-01, -3.3124e-02, -3.0892e-01,\n",
       "          9.3524e-02,  1.8248e-01, -1.5747e-01,  3.3168e-01,  7.1896e-02],\n",
       "        [ 4.0103e-01,  5.6167e-02,  4.7031e-01, -4.4262e-01, -5.9264e-01,\n",
       "          1.4343e-01,  6.5997e-01, -8.8853e-01,  5.8428e-01, -4.1313e-01],\n",
       "        [-4.4681e-01,  8.6035e-01,  1.1983e+00, -9.1662e-01, -8.6516e-02,\n",
       "         -3.5220e-01,  3.8914e-01, -9.6486e-01,  1.3196e-02, -6.4043e-01],\n",
       "        [ 2.1481e-01, -3.3318e-01,  4.8984e-01, -4.6199e-01, -6.1180e-01,\n",
       "          6.7169e-01,  1.3465e-01, -2.1533e+00,  5.9294e-01, -1.5316e-02],\n",
       "        [ 5.1856e-01, -5.0949e-02,  2.5940e-01, -2.9196e-01, -1.3549e+00,\n",
       "          3.1741e-01,  2.4084e-01,  8.8853e-02, -1.8847e-01,  8.8867e-01],\n",
       "        [ 1.9836e-01, -8.3500e-01, -5.3353e-01, -1.0741e+00, -1.7013e+00,\n",
       "          4.3409e-01,  5.6275e-01, -3.5663e-01,  9.6370e-01,  4.6091e-01],\n",
       "        [ 3.9331e-01,  8.1920e-02,  6.4324e-01, -3.8385e-01, -8.6158e-01,\n",
       "          1.4922e-02, -2.3944e-01, -3.8467e-01,  6.0430e-01,  2.3442e-01],\n",
       "        [ 1.1099e+00,  2.3470e-01, -2.5090e-01, -6.4868e-01, -3.8346e-01,\n",
       "          6.7207e-01, -7.9066e-02, -1.0051e+00,  9.0903e-01, -1.1349e-01],\n",
       "        [ 3.6082e-01,  2.2881e-01,  1.5975e-01,  2.1260e-01, -3.0847e-01,\n",
       "         -4.2898e-01,  2.5018e-01,  7.1059e-02, -7.6640e-01,  1.5480e-01],\n",
       "        [ 9.4272e-02, -2.5211e-01, -1.8019e-01,  1.0762e-01, -1.0107e+00,\n",
       "          3.7289e-01,  4.7816e-01, -1.4273e-01,  2.5107e-01, -3.9259e-01],\n",
       "        [ 1.3956e-02, -9.3954e-01,  5.0653e-01, -1.0468e+00, -8.4116e-01,\n",
       "          2.5938e-01,  1.0498e-01, -9.1082e-01,  9.7076e-01,  3.8609e-01],\n",
       "        [ 1.8935e-01,  2.3752e-01, -1.8916e-01, -7.9345e-01, -5.5220e-01,\n",
       "          3.5964e-03, -1.1048e-01, -4.7242e-01,  2.4307e-01, -1.8316e-01],\n",
       "        [ 4.6578e-02, -8.2629e-03,  2.0578e-01, -3.8828e-01, -4.5726e-01,\n",
       "         -2.0338e-01,  2.2487e-01, -6.9916e-01,  2.0042e-01, -7.6543e-02],\n",
       "        [-2.0113e-01,  5.3134e-01,  1.3478e+00, -8.9817e-02, -5.1530e-01,\n",
       "          6.1496e-02,  1.0210e-01, -1.3718e+00,  3.1026e-01,  4.1156e-02],\n",
       "        [-3.8609e-01,  9.3769e-01,  3.3111e-01,  3.0830e-01, -6.9632e-01,\n",
       "         -2.1909e-01,  4.8218e-01,  4.4859e-01, -3.3390e-01, -3.1107e-01],\n",
       "        [ 7.0910e-01, -3.2962e-02,  7.1738e-01, -1.2840e+00, -5.1598e-01,\n",
       "         -1.6349e-02, -4.5894e-01, -1.5101e+00,  5.2429e-01, -5.2271e-01],\n",
       "        [ 1.5746e-01,  2.4839e-01,  9.5855e-01, -2.9509e-01, -6.5264e-01,\n",
       "          2.2813e-01,  4.8192e-01, -6.7256e-01,  1.6087e-01,  3.0581e-01],\n",
       "        [ 1.7947e-01,  3.3546e-01,  7.0486e-01, -5.9738e-02, -3.3772e-01,\n",
       "         -2.3242e-02,  2.5631e-01, -2.5523e-01, -2.9719e-01, -3.5690e-01],\n",
       "        [ 3.7485e-01,  3.1680e-01,  2.8625e-01, -4.7171e-01, -1.5124e-01,\n",
       "          3.0567e-01,  4.2120e-01, -1.4655e+00,  7.6815e-01,  4.4285e-01],\n",
       "        [ 3.4348e-02,  3.8924e-01,  5.9041e-01, -1.2569e-01, -1.0029e-01,\n",
       "          3.6982e-01, -3.0862e-01, -9.5943e-01, -1.0139e-01, -1.4422e-01],\n",
       "        [ 2.3242e-01,  5.2889e-02,  1.5296e-01, -3.0757e-01, -5.5797e-01,\n",
       "          3.9645e-01,  4.1478e-01, -7.7230e-01, -3.7030e-01, -1.1337e-01],\n",
       "        [-1.9397e-02,  3.5630e-01,  7.2973e-02, -5.9468e-01, -4.9518e-01,\n",
       "          9.9645e-02,  4.1449e-01, -9.8794e-01,  3.3893e-01,  2.0270e-01],\n",
       "        [ 2.2026e-01, -5.9502e-01,  1.2830e+00, -2.1001e-01, -6.8304e-01,\n",
       "          5.0970e-01,  4.8694e-01, -1.3848e+00,  8.3214e-01, -1.2585e-01],\n",
       "        [-7.1880e-02, -6.3777e-02,  1.6970e-01, -1.0732e-01, -1.1723e-01,\n",
       "          3.1030e-01,  1.3839e-01,  9.7230e-02,  1.4619e-01, -2.5363e-01],\n",
       "        [ 5.6753e-01, -3.1231e-01,  4.2350e-01, -7.5654e-01,  2.0725e-01,\n",
       "         -5.0223e-01,  4.8362e-01, -1.7076e+00,  5.1936e-01,  2.7201e-01],\n",
       "        [ 3.1428e-01,  1.6314e-01,  3.3164e-01,  5.8246e-02, -1.0182e+00,\n",
       "         -2.5288e-01,  3.6301e-01, -5.6234e-01,  1.7763e-01,  4.5735e-02],\n",
       "        [ 2.7909e-01,  2.0698e-01,  1.8046e-01, -3.7858e-01, -4.8407e-01,\n",
       "          5.5055e-02,  2.9689e-01, -5.1911e-01,  5.1494e-01, -4.4952e-02],\n",
       "        [ 4.2548e-01,  3.4613e-01, -1.6224e-04,  1.2789e-01, -1.2749e-01,\n",
       "         -2.8145e-01, -8.4741e-02, -6.4173e-01,  1.0917e-01, -4.2579e-01],\n",
       "        [ 5.7848e-01,  5.2139e-01, -3.3262e-01, -2.2688e-01, -1.0340e-01,\n",
       "          1.8322e-02, -2.8489e-01, -4.3261e-01,  3.0471e-01, -5.2067e-01],\n",
       "        [ 7.5692e-01, -4.3376e-01,  1.0057e-01, -6.4051e-01, -9.5045e-01,\n",
       "          5.9767e-01,  5.2595e-01,  3.0212e-01,  3.2282e-01, -6.9860e-01],\n",
       "        [ 1.1911e-01,  1.2785e-01, -5.0162e-01, -4.0221e-01, -4.1516e-01,\n",
       "         -2.6669e-01, -4.0934e-02,  1.5452e-01, -1.5618e-01, -1.6370e-01],\n",
       "        [ 4.6447e-01,  6.9226e-01, -3.0880e-01, -3.3297e-01, -1.3207e+00,\n",
       "          7.2422e-01, -5.6017e-01, -4.2979e-01,  2.1888e-01,  3.0074e-01],\n",
       "        [-1.1293e-01,  5.0148e-01,  7.8508e-01, -1.3594e-01, -5.9755e-01,\n",
       "          7.2888e-01,  7.4254e-02, -8.3520e-01,  3.4912e-01, -7.2230e-03],\n",
       "        [ 4.3126e-01,  3.4919e-01,  3.7860e-01, -4.8768e-01, -6.6005e-01,\n",
       "         -8.4318e-03,  6.3355e-01,  2.4231e-01, -4.4801e-02, -6.0660e-01],\n",
       "        [-7.2989e-02,  1.0738e+00,  7.8725e-01, -1.8046e-01, -5.6994e-01,\n",
       "         -1.5800e-01, -5.3648e-02, -7.7330e-01,  1.1657e-02, -1.1230e-01],\n",
       "        [ 1.1174e-02, -6.7572e-01, -1.3578e-01, -1.1596e-01, -6.8548e-01,\n",
       "         -1.4135e-01,  4.0919e-01, -1.9256e-01, -2.7306e-01,  2.7254e-01],\n",
       "        [ 4.6670e-01, -9.4818e-02,  1.3532e-01, -6.2689e-01, -4.3021e-01,\n",
       "          2.8965e-01,  1.3096e-01, -3.3021e-01,  6.5513e-01,  2.4240e-02],\n",
       "        [-6.5517e-01,  8.4937e-01,  1.0654e+00, -8.6918e-01, -5.5933e-01,\n",
       "         -2.0762e-01,  8.6621e-02, -1.6380e+00,  1.3225e-01, -1.2384e-01],\n",
       "        [ 2.0785e-01,  7.0515e-02,  8.2437e-01, -1.0024e+00, -9.7514e-01,\n",
       "         -1.1526e-01, -4.0237e-01, -2.2454e+00,  7.6123e-01, -2.1472e-01],\n",
       "        [ 5.8904e-01, -2.2295e-01,  2.0383e-01, -1.5780e-01, -3.8774e-01,\n",
       "          2.8903e-01,  3.7201e-01,  1.8742e-01, -5.2845e-02, -1.1387e-01],\n",
       "        [ 3.8792e-01,  7.0185e-01,  3.3403e-01,  2.2805e-01, -3.3490e-01,\n",
       "         -6.0480e-01, -5.0400e-01, -2.3055e-01,  1.4691e-01, -3.7832e-01],\n",
       "        [ 6.4784e-03, -3.6591e-01,  9.4893e-01,  4.3551e-02, -6.6647e-01,\n",
       "         -5.8266e-02,  1.0094e-01, -5.2145e-01,  8.6902e-02, -2.8926e-01],\n",
       "        [ 1.1739e-01,  1.4101e-01, -3.6682e-01, -1.0955e-01, -3.0089e-01,\n",
       "          2.8302e-01,  2.1938e-01,  2.7354e-01,  6.9971e-02, -7.3396e-01],\n",
       "        [ 9.2493e-01,  2.1008e-01,  2.1503e-01, -1.1455e-02, -9.0063e-01,\n",
       "         -1.7255e-01, -2.2658e-01, -6.2241e-01,  5.6811e-02, -1.5759e-01],\n",
       "        [ 4.9895e-01,  3.1732e-01, -4.6233e-01, -3.0625e-01, -7.8171e-01,\n",
       "         -3.1472e-01,  1.9051e-01,  2.5795e-01,  3.8981e-02, -9.3106e-01],\n",
       "        [ 8.1228e-02, -4.8455e-02,  9.6222e-01, -6.7088e-01, -3.5290e-01,\n",
       "         -1.9467e-01,  5.8539e-02, -1.8053e+00,  4.2577e-01, -1.0568e-01],\n",
       "        [ 1.8365e-01,  3.2923e-01,  9.6923e-01, -2.6403e-01, -2.5690e-01,\n",
       "         -5.4367e-01, -1.0205e-01, -6.0448e-01,  4.1665e-01, -2.7063e-01],\n",
       "        [ 3.5931e-01,  2.9554e-01, -1.4179e-01, -4.7073e-01, -4.0598e-01,\n",
       "          6.1683e-01,  1.7442e-01, -7.9090e-01,  5.6462e-02, -2.5257e-01],\n",
       "        [ 1.5461e-03,  7.0546e-02,  5.3084e-01,  1.7464e-01, -7.3976e-01,\n",
       "          1.6714e-01,  5.8945e-01, -3.6819e-01,  2.5238e-01, -2.0162e-01],\n",
       "        [ 4.9578e-02,  1.9588e-01,  5.7798e-02, -4.2398e-01, -1.6467e-01,\n",
       "          2.7678e-01,  5.4180e-02, -7.9094e-01,  1.6166e-01, -7.9139e-02],\n",
       "        [ 2.0710e-01, -1.0172e+00,  4.9138e-01, -1.3456e-01, -7.9707e-01,\n",
       "          5.0766e-01,  4.0341e-01, -5.0461e-01,  4.7966e-01, -2.6074e-01],\n",
       "        [-4.3152e-02, -3.4312e-01,  1.2097e-01,  1.6121e-01, -3.8978e-01,\n",
       "          8.4304e-01,  1.1285e-01, -3.3393e-01,  1.9978e-01,  8.8694e-02],\n",
       "        [ 1.4719e-01,  3.7636e-01,  1.8902e-01, -1.2166e-01, -2.7759e-01,\n",
       "          2.5819e-01, -5.4053e-01, -7.9250e-01,  1.5917e-01, -3.8117e-01],\n",
       "        [ 1.5523e-01,  7.4020e-02,  5.1661e-02, -7.4569e-02, -4.2943e-01,\n",
       "          2.9229e-01,  5.9035e-01, -3.3184e-01, -7.2267e-02, -3.5883e-01],\n",
       "        [ 3.3221e-01, -1.3277e+00,  2.9098e-01, -1.9077e-01, -4.6217e-01,\n",
       "          7.5544e-01,  3.3596e-01, -7.1075e-01,  2.8700e-01,  4.6062e-01],\n",
       "        [ 4.0590e-01,  1.3321e-01, -4.2855e-01, -3.6303e-01, -4.3593e-01,\n",
       "         -2.3509e-02,  8.1907e-02, -6.9943e-01,  2.3244e-01, -5.1307e-01],\n",
       "        [ 8.4905e-01,  1.7620e-01,  6.5768e-01,  2.2936e-01, -1.2318e+00,\n",
       "         -3.9809e-01, -2.0799e-01, -4.1064e-01, -1.8429e-01, -3.7256e-01],\n",
       "        [ 1.6090e-01,  4.9732e-01,  3.5794e-01,  1.3066e-01, -9.4938e-01,\n",
       "         -2.2947e-01,  1.8373e-01,  4.1271e-01, -5.4934e-01, -2.5414e-01],\n",
       "        [-2.6321e-01,  7.0320e-01,  9.6777e-01, -3.5989e-01, -1.5298e-01,\n",
       "          1.5244e-01, -3.5937e-01, -6.1248e-01, -1.9854e-01, -4.7515e-01],\n",
       "        [-7.8694e-02,  4.5109e-01, -6.9954e-02,  1.9203e-01, -4.5687e-01,\n",
       "         -4.3338e-02, -1.7277e-01, -1.8092e-01,  5.5942e-02, -8.1950e-01],\n",
       "        [ 2.3794e-01,  3.8154e-01,  3.9003e-01,  3.0044e-01, -4.0097e-01,\n",
       "         -3.5692e-01,  4.0091e-01, -8.0790e-01, -2.0915e-01,  8.0147e-02],\n",
       "        [ 3.1478e-01, -1.3802e-01, -1.9781e-01, -1.2763e-01, -8.2721e-02,\n",
       "         -1.9194e-01, -4.7709e-03, -8.8141e-01,  8.0116e-01, -5.7628e-01],\n",
       "        [-3.3856e-01,  1.2083e-01,  7.5092e-01, -7.6592e-01,  3.2104e-01,\n",
       "         -4.8540e-01,  2.9845e-01, -1.0145e+00,  3.5303e-01, -5.9237e-01],\n",
       "        [ 2.8509e-01,  1.3771e-01,  2.7575e-01, -1.5795e-01, -3.0654e-01,\n",
       "          6.5006e-01, -2.7991e-02, -5.0222e-01,  1.0678e-01, -4.8763e-01],\n",
       "        [ 3.4973e-01,  6.2981e-01, -2.6448e-01, -3.6973e-01, -6.3968e-01,\n",
       "          4.9327e-01, -1.7661e-01, -7.7904e-01, -2.0861e-01, -2.1311e-01],\n",
       "        [-4.3405e-03,  4.7804e-01, -4.2282e-01, -2.3761e-01, -8.3319e-01,\n",
       "          7.2633e-01,  2.3113e-01, -5.2643e-01,  1.0244e-01, -2.0120e-01],\n",
       "        [-8.4820e-01,  1.0006e+00,  9.2786e-01, -4.6581e-01,  2.2664e-01,\n",
       "         -4.9354e-01, -2.9893e-01, -4.3156e-01,  1.5170e-01, -3.6287e-01],\n",
       "        [-3.8571e-01,  7.0290e-01,  1.5988e-01, -2.2704e-01, -2.3454e-01,\n",
       "         -3.0094e-01,  1.2566e-01, -7.0871e-01, -2.7867e-01, -3.3591e-01]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just for Ben to run one iteraiton of hypernet forward func to see what's going on\n",
    "# under the hood.\n",
    "inputs, _ = next(iter(train_loader))\n",
    "inputs = inputs.view(-1, 28*28).to(device)\n",
    "z = encoder(inputs).to(device)\n",
    "module(inputs, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for if you want to test out our HyperNet pipeline with a random input embedding z (that is not generated from an encoder)\n",
    "# z = torch.randn(32, 3)\n",
    "# x, y = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(module.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 108/600 [00:02<00:08, 55.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   101] loss: 2.300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 210/600 [00:03<00:07, 53.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   201] loss: 2.091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 306/600 [00:05<00:05, 53.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   301] loss: 1.913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 408/600 [00:07<00:03, 53.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   401] loss: 1.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 504/600 [00:09<00:01, 52.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   501] loss: 1.550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:11<00:00, 53.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   601] loss: 1.374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 107/600 [00:01<00:08, 58.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,   101] loss: 1.221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 210/600 [00:03<00:07, 54.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,   201] loss: 1.080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 306/600 [00:05<00:05, 53.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,   301] loss: 0.965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 408/600 [00:07<00:03, 54.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,   401] loss: 0.868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 510/600 [00:09<00:01, 52.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,   501] loss: 0.795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:10<00:00, 55.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,   601] loss: 0.754\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):  # loop over the dataset num_epochs times\n",
    "    running_loss = 0.0\n",
    "    i = 0  # Just keeps track of which batch we're on until we get to batch 500, in which case we print\n",
    "           # out the aggregated loss over the 500 batches and reset (i.e. allows us to just print out the\n",
    "           # average loss every 500 batches instead of every batch)\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs = inputs.view(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        z = encoder(inputs).to(device)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        # outputs = module(inputs, weights).reshape(-1, 10)\n",
    "        outputs = module(inputs, z).reshape(-1, 10)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        i += 1\n",
    "        if i % 100 == 0:    # print every 500 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 10.08 %\n"
     ]
    }
   ],
   "source": [
    "# Test the accuracy of our trained model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        z = encoder(inputs).to(device)\n",
    "        outputs = module(inputs, z).reshape(-1, 10)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d75777fdd50f2f9802efe22d891fef8171690b7147965fffec272f010a12e4cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
